---
title: Correlation tables annoy me
author: Nick Vasile
date: '2019-02-14'
slug: correlation-tables-annoy-me
categories: []
tags: []
---



<div id="im-lazy" class="section level2">
<h2>I’m lazy</h2>
<p>And so are most people.</p>
<p>I like to focus on as few things as possible at any given moment and correlation tables get in the way of that.</p>
</div>
<div id="correlation-tables-contain-useless-data" class="section level2">
<h2>Correlation tables contain useless data</h2>
<pre class="r"><code>corrs &lt;- cor(mtcars)

corrplot::corrplot(corrs, order = &quot;hclust&quot;)</code></pre>
<div class="figure">
<img src="/images/correlation_table_duh.png" />

</div>
<p>I don’t need to be reminded that each predictor is perfectly correlated with itself.</p>
<p>I guess this could be useful because the border it forms warns me that I am about to see the same exact information for the second time.</p>
<p>Which brings me to…</p>
</div>
<div id="correlation-tables-duplicate-information" class="section level2">
<h2>Correlation tables duplicate information</h2>
<div class="figure">
<img src="/images/correlation_table_repeat.png" />

</div>
</div>
<div id="a-solution-part-1-eliminate-the-obvious" class="section level2">
<h2>A solution part 1: Eliminate the obvious</h2>
<p>After taking a quick look at the table (just the first half :) )to get an idea of the largest correlation values I establish a cut-off point to look at the pairs with the strongest values.</p>
<p>I also remove values where a predictor is correlated with itself.</p>
<pre class="r"><code>cut_off &lt;- 0.8

corrs &lt;- cor(mtcars)

# probably *shouldn&#39;t* be using melt() here b/c reshape2 is
# depreciated but it 
# is easy and I like easy because I am lazy
correlated &lt;- reshape2::melt(corrs) %&gt;% 
    dplyr::filter(value &gt; cut_off,
           #remove entries for a variable correlated to itself
           Var1 != Var2) %&gt;% 
    dplyr::arrange(desc(value)) #not necessary just sorting to demo pairwise dups

correlated</code></pre>
<pre><code>##   Var1 Var2     value
## 1 disp  cyl 0.9020329
## 2  cyl disp 0.9020329
## 3   wt disp 0.8879799
## 4 disp   wt 0.8879799
## 5   hp  cyl 0.8324475
## 6  cyl   hp 0.8324475</code></pre>
<p>Notice each successive pair of rows is a pairwise duplicate.</p>
</div>
<div id="a-solution-part-2-eliminate-the-duplicates" class="section level2">
<h2>A solution part 2: Eliminate the duplicates</h2>
<pre class="r"><code>correlated %&gt;% 
    dplyr::mutate(
        combo = dplyr::if_else(
            # &gt; and &lt; comparison doesnt work with factors
            as.character(Var1) &gt; as.character(Var2), #if
                               stringr::str_c(Var1, Var2), #then 
                               stringr::str_c(Var2, Var1) #else
        )
    ) %&gt;% 
    dplyr::distinct(combo, .keep_all= TRUE) %&gt;% 
    dplyr::select(-combo)</code></pre>
<pre><code>##   Var1 Var2     value
## 1 disp  cyl 0.9020329
## 2   wt disp 0.8879799
## 3   hp  cyl 0.8324475</code></pre>
<p>There you have it - only the unique predictor pairs!</p>
<p>I arrived at this solution after finding this <a href="https://stackoverflow.com/a/25595218">stackoverflow post</a></p>
</div>
<div id="if-you-found-this-useful" class="section level2">
<h2>If you found this useful</h2>
<p>You may like <a href="https://www.nickvasile.com/cheat/">my cheat sheet</a>.</p>
</div>
